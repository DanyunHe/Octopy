---
layout: default
title: {{ site.name }}
---

<div id="home">
  <h1>Blog Posts</h1>
  <ul class="posts">
    {% for post in site.posts %}
      <li><span>{{ post.date | date_to_string }}</span> &raquo; <a href="{{ site.baseurl }}{{ post.url }}">{{ post.title }}</a></li>
    {% endfor %}
  </ul>
  <h1>Introduction</h1>
    <ul class="posts">
      {% for post in site.posts %}
        <li><span>Federated Learning</li>
          <p>
            Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches.
            Federated Learning is a decentralized model training approach by leaving the training data distributed on the mobile devices, while learning a shared model by aggregating locally-computed updates.    
          </p>
          <p>
            <b>The benefits of Federated Learning:</b>
            <ul>
              <li>Privacy: with no direct access to personal data</li>
              <li>Efficiency: local training on multiple local devices</li>
              <li>Scalable: Gboard from Google already adopted this framework</li>
            </ul>
            
          </p>
          <p>
            <b>The procudure of Federated Learning:</b>
            <ul>
              <li>The server opens a new round of training </li>
              <li>The clients download the latest version of the model from the server and update the model using their local data. Those updates are sent to the server </li>
              <li>The server gathers all updates and applies Federated Averaging to improve the shared model </li>
              <li>The shared model is now ready for all clients to download for the next round </li>
            </ul> 
          </p>

      {% endfor %}
    </ul>
    <ul class="posts">
      {% for post in site.posts %}
        <li><span>Problem</li>
          <p>
            For researchers, an efficient simulation paradigm for Federated Learning is still lacking. Researchers need a testbed to validate performance. It is not feasible to buy 1e4 devices like Google to build such a testing environment, so researchers often use workstation with several GPUs to simulate behaviors of tens of thousands of clients. In practice, numerous local models are trained on several GPUs, and local weights are copied and updated to global model on CPU in every round of training. Since data are distributed in each client, the communication costs dominate in the federated model. And big compute is needed for weights aggregation. The current implementation is serial code, which will read weights from clients one by one, this implementation is very inefficient, the utilization of GPU is lower than 10%. Therefore, we propose a parallel implementation for this model. 
          </p>
      {% endfor %}
    
  <h1>Model Design</h1>
  <h1>Performance</h1>
</div>
